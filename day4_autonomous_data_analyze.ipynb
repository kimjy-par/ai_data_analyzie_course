{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4nSzmTNkPz06Ff2p5ZKbX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimjy-par/ai_data_analyzie_course/blob/main/day4_autonomous_data_analyze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bf0ab92"
      },
      "source": [
        "# YouTube 채널 분석 실습 개요\n",
        "\n",
        "* * *\n",
        "\n",
        "본 실습은 YouTube Data API와 Google Gemini API를 활용하여 특정 YouTube 채널의 영상 및 댓글 데이터를 수집하고 분석하는 과정을 다룹니다. 이전 실습에서 수동으로 진행했던 각 단계를, **미리 작성된 함수를 활용하여 간단하게 자동으로 실행하고 그 결과를 확인하는 데 중점을 둡니다.** 수집된 데이터를 바탕으로 AI 모델을 통해 심층적인 분석 리포트를 생성하고, 이를 통해 콘텐츠 기획 및 채널 운영 전략에 대한 실질적인 인사이트를 얻는 것을 목표로 합니다.\n",
        "\n",
        "# 실습 목표:\n",
        "\n",
        "* * *\n",
        "\n",
        "1.  **미리 작성된 함수를 활용하여 각 단계 별로 처리된 결과를 자동으로 확인하고 전체 분석 파이프라인을 실행할 수 있다.**\n",
        "2.  **수집 설정 (영상 개수, 댓글 개수 등) 및 AI 프롬프트를 수정하여 분석 결과가 어떻게 달라지는지 확인하고 결과를 분석할 수 있다.**\n",
        "\n",
        "\n",
        "# 실습 전 준비사항:\n",
        "\n",
        "* * *\n",
        "\n",
        "(이전 시간과 동일)\n",
        "1. YouTube Data API 키\n",
        "2. Google Gemini API 키\n",
        "3. 분석하고 싶은 유튜브 채널\n",
        "\n",
        "# 실습 절차\n",
        "* * *\n",
        "\n",
        "1.  **필요 라이브러리 설치 및 환경 설정**: YouTube API 및 Gemini API 사용을 위한 라이브러리를 설치하고 API 키를 설정합니다. Google Drive 연결을 통해 분석 결과를 저장할 환경을 준비합니다.\n",
        "2.  **YouTube 데이터 수집**: 특정 채널의 동영상 목록을 가져오고, 각 영상에 대한 댓글을 수집합니다. 수집 범위 (영상 개수, 댓글 개수 등)를 설정하여 원하는 데이터를 확보합니다.\n",
        "3.  **데이터 전처리 및 저장**: 수집된 영상 및 댓글 데이터를 분석 가능한 형태로 가공하고, 추후 활용을 위해 파일로 저장합니다.\n",
        "4.  **AI 분석 리포트 생성**: 전처리된 데이터를 바탕으로 Gemini AI 모델에 분석을 요청하고, AI가 생성한 분석 리포트를 확인합니다.\n",
        "\n",
        "* * *\n",
        "\n",
        "**참고사항:**\n",
        "\n",
        "*   실습 중 API 할당량에 유의하여 진행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylkFs9pIRZaE",
        "outputId": "127032a4-253f-4222-956d-43a63e30784e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2025.9.5-py3-none-any.whl.metadata (177 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/177.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m122.9/177.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (2.181.0)\n",
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.34.0)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.12/dist-packages (1.9.4)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (0.30.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (2.38.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (2.25.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (4.2.0)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.10.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.11.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.32.4)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai) (8.5.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from wordcloud) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from wordcloud) (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from wordcloud) (3.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
            "Downloading yt_dlp-2025.9.5-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp\n",
            "Successfully installed yt-dlp-2025.9.5\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 필요한 라이브러리를 모두 설치합니다 (한번만 실행)\n",
        "!pip install yt-dlp google-api-python-client google-genai wordcloud\n",
        "\n",
        "# 필요한 모든 라이브러리를 한번에 불러옵니다\n",
        "import os                      # 파일과 폴더 관리용\n",
        "import json                    # JSON 데이터 처리용\n",
        "import time                    # 시간 지연용\n",
        "import pandas as pd            # 데이터 분석용 (엑셀과 비슷)\n",
        "import matplotlib.pyplot as plt # 그래프 그리기용\n",
        "import numpy as np             # 수학 계산용\n",
        "from collections import Counter # 빈도 계산용\n",
        "import re                      # 텍스트 처리용\n",
        "from datetime import datetime, timedelta  # 날짜 계산용\n",
        "import subprocess              # 외부 프로그램 실행용\n",
        "\n",
        "\n",
        "# Google Drive를 연결합니다 (결과 저장용)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# YouTube 데이터를 가져오는 라이브러리\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "\n",
        "\n",
        "# AI 분석용 라이브러리\n",
        "from google import genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 환경 설정\n",
        "YOUTUBE_API_KEY = \"유튜브 API KEY\"  # YouTube Data API 키\n",
        "GEMINI_API_KEY = \"GEMINI API KEY\"    # Gemini AI 키"
      ],
      "metadata": {
        "id": "1uVHRXzpqmgF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_VIDEOS = 10             # 분석할 영상 개수 (10개)\n",
        "MAX_RAW_VIDEOS = 500       # 인기순 혹은 최신순 정렬을 위해 가져올 비디오 개수\n",
        "MAX_COMMENTS_PER_VIDEO = 20 # 영상당 댓글 개수 (20개)\n",
        "\n",
        "# 결과 저장할 폴더 설정\n",
        "SAVE_PATH = \"/content/drive/MyDrive/youtube_analysis/\"\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)  # 폴더가 없으면 자동 생성\n",
        "\n",
        "print(\"설정 완료!\")\n",
        "print(f\"저장 위치: {SAVE_PATH}\")\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# API 연결 설정\n",
        "# =================================================================\n",
        "\n",
        "# YouTube Data API 연결\n",
        "try:\n",
        "    youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)\n",
        "    print(\"YouTube API 연결 성공!\")\n",
        "except Exception as e:\n",
        "    print(f\"YouTube API 연결 실패: {e}\")\n",
        "    print(\"API 키를 확인해주세요\")\n",
        "\n",
        "# Gemini AI 연결\n",
        "try:\n",
        "    os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n",
        "    genai_client = genai.Client()\n",
        "    print(\"Gemini AI 연결 성공!\")\n",
        "except Exception as e:\n",
        "    print(f\"Gemini AI 연결 실패: {e}\")\n",
        "    print(\"API 키를 확인해주세요\")\n",
        "\n",
        "\n",
        "\n",
        "def get_channel_id_by_handle(handle):\n",
        "    \"\"\"채널 핸들로 실제 채널 ID를 찾는 함수\"\"\"\n",
        "    try:\n",
        "        handle = handle.replace('@', '')\n",
        "        request = youtube.search().list(\n",
        "            part='snippet',\n",
        "            q=handle,\n",
        "            type='channel',\n",
        "            maxResults=1\n",
        "        )\n",
        "        response = request.execute()\n",
        "        if response['items']:\n",
        "            return response['items'][0]['snippet']['channelId']\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"채널 ID 찾기 에러: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_all_channel_videos(youtube, channel_id, limit=MAX_RAW_VIDEOS):\n",
        "    \"\"\"\n",
        "    지정된 채널의 동영상 상세 정보를 지정된 최대 개수(limit)만큼만 가져옵니다.\n",
        "\n",
        "    Args:\n",
        "        youtube: YouTube API 서비스 객체\n",
        "        channel_id (str): 조회할 채널의 ID\n",
        "        limit (int): 가져올 최대 동영상 개수\n",
        "\n",
        "    Returns:\n",
        "        list: 채널의 동영상 정보가 담긴 리스트\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # (이전과 동일) 채널의 '업로드' 재생목록 ID 가져오기\n",
        "        channel_response = youtube.channels().list(part='contentDetails', id=channel_id).execute()\n",
        "        uploads_playlist_id = channel_response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
        "\n",
        "        video_ids = []\n",
        "        next_page_token = None\n",
        "\n",
        "        # while 루프 시작\n",
        "        while True:\n",
        "            playlist_response = youtube.playlistItems().list(\n",
        "                part='contentDetails',\n",
        "                playlistId=uploads_playlist_id,\n",
        "                maxResults=50,\n",
        "                pageToken=next_page_token\n",
        "            ).execute()\n",
        "\n",
        "            video_ids.extend([item['contentDetails']['videoId'] for item in playlist_response['items']])\n",
        "\n",
        "            # --- ✨ 핵심 수정 부분 ---\n",
        "            # 1. 현재까지 가져온 동영상 개수가 limit을 넘었는지 확인\n",
        "            if len(video_ids) >= limit:\n",
        "                break # 넘었다면 루프 중단\n",
        "\n",
        "            next_page_token = playlist_response.get('nextPageToken')\n",
        "\n",
        "            # 2. 다음 페이지가 없어도 루프 중단\n",
        "            if not next_page_token:\n",
        "                break\n",
        "\n",
        "        # --- ✨ 가져온 ID 개수에 맞춰 자르기 ---\n",
        "        # limit을 초과해서 가져왔을 수 있으므로 정확히 limit 개수만큼만 잘라냅니다.\n",
        "        limited_video_ids = video_ids[:limit]\n",
        "\n",
        "        all_videos = []\n",
        "        for i in range(0, len(limited_video_ids), 50):\n",
        "            videos_response = youtube.videos().list(\n",
        "                part='snippet,statistics,contentDetails',\n",
        "                id=','.join(limited_video_ids[i:i+50])\n",
        "            ).execute()\n",
        "            all_videos.extend(videos_response['items'])\n",
        "\n",
        "        return all_videos\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"데이터 수집 중 에러 발생: {e}\")\n",
        "        return []\n",
        "\n",
        "def analyze_channel_videos(videos, sort_by='popular', start_date=None, end_date=None, limit=MAX_VIDEOS):\n",
        "    \"\"\"\n",
        "    가져온 동영상 리스트를 주어진 조건에 따라 필터링하고 정렬합니다.\n",
        "\n",
        "    Args:\n",
        "        videos (list): 분석할 전체 동영상 리스트\n",
        "        sort_by (str): 'popular' (인기순), 'latest' (최신순) 중 하나\n",
        "        start_date (str): 'YYYY-MM-DD' 형식의 시작 날짜\n",
        "        end_date (str): 'YYYY-MM-DD' 형식의 종료 날짜\n",
        "\n",
        "    Returns:\n",
        "        list: 조건에 맞게 처리된 동영상 리스트\n",
        "    \"\"\"\n",
        "    processed_videos = videos\n",
        "\n",
        "    # 1. 날짜 필터링\n",
        "    if start_date and end_date:\n",
        "        start_dt = datetime.fromisoformat(start_date + 'T00:00:00Z')\n",
        "        end_dt = datetime.fromisoformat(end_date + 'T23:59:59Z')\n",
        "        processed_videos = [\n",
        "            v for v in processed_videos\n",
        "            if start_dt <= datetime.fromisoformat(v['snippet']['publishedAt'].replace('Z', '+00:00')) <= end_dt\n",
        "        ]\n",
        "\n",
        "    # 2. 정렬\n",
        "    if sort_by == 'popular':\n",
        "        processed_videos = sorted(\n",
        "            processed_videos,\n",
        "            key=lambda v: int(v['statistics'].get('viewCount', 0)),\n",
        "            reverse=True\n",
        "        )\n",
        "    elif sort_by == 'latest':\n",
        "        processed_videos = sorted(\n",
        "            processed_videos,\n",
        "            key=lambda v: v['snippet']['publishedAt'],\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "    return processed_videos[:limit]\n",
        "\n",
        "def convert_dataframe_from_video_data(videos):\n",
        "  videos_data = []\n",
        "  for video in videos:\n",
        "    video_data = {\n",
        "        \"video_id\": video.get(\"id\"),\n",
        "        \"title\": video.get(\"snippet\").get(\"title\"),\n",
        "        \"channel_title\": video.get(\"snippet\").get(\"channelTitle\"),\n",
        "        \"upload_date\": video.get(\"snippet\").get(\"publishedAt\"),\n",
        "        \"view_count\": video.get(\"statistics\").get(\"viewCount\"),\n",
        "        \"like_count\": video.get(\"statistics\").get(\"likeCount\"),\n",
        "        \"comment_count\": video.get(\"statistics\").get(\"commentCount\"),\n",
        "        \"duration\": video.get(\"contentDetails\").get(\"duration\"),\n",
        "        \"description\": video.get(\"snippet\").get(\"description\"),\n",
        "        \"collected_at\": datetime.now().isoformat()\n",
        "    }\n",
        "    videos_data.append(video_data)\n",
        "\n",
        "  videos_df = pd.DataFrame(videos_data)\n",
        "  return videos_df\n",
        "\n",
        "def collect_video_comments(videos_df, youtube=youtube, max_comments_per_video=MAX_COMMENTS_PER_VIDEO):\n",
        "    \"\"\"\n",
        "    주어진 동영상 목록(DataFrame)에 대해 댓글을 수집하여 DataFrame으로 반환합니다.\n",
        "\n",
        "    Args:\n",
        "        youtube: YouTube API 서비스 객체\n",
        "        videos_df (pd.DataFrame): 'video_id', 'title' 컬럼을 포함한 데이터프레임\n",
        "        max_comments_per_video (int): 영상당 수집할 최대 댓글 수\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: 수집된 모든 댓글 정보가 담긴 데이터프레임\n",
        "    \"\"\"\n",
        "    if videos_df.empty:\n",
        "        print(\"분석할 영상이 없어 댓글 수집을 건너뜁니다.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    all_comments = []\n",
        "    print(\"\\n=================================================================\")\n",
        "    print(\"영상 댓글 수집 시작\")\n",
        "    print(\"=================================================================\")\n",
        "\n",
        "    for idx, video in videos_df.iterrows():\n",
        "        video_id = video['video_id']\n",
        "        video_title = video['title']\n",
        "        print(f\"\\n[{idx+1}/{len(videos_df)}] '{video_title[:40]}...' 댓글 수집 중...\")\n",
        "\n",
        "        try:\n",
        "            comments_request = youtube.commentThreads().list(\n",
        "                part='snippet',\n",
        "                videoId=video_id,\n",
        "                maxResults=max_comments_per_video,\n",
        "                order='relevance'  # 인기순 (또는 'time'으로 최신순)\n",
        "            )\n",
        "            comments_response = comments_request.execute()\n",
        "\n",
        "            video_comments = []\n",
        "            for item in comments_response['items']:\n",
        "                top_comment = item['snippet']['topLevelComment']['snippet']\n",
        "                comment_data = {\n",
        "                    'video_id': video_id,\n",
        "                    'video_title': video_title,\n",
        "                    'comment_id': item['snippet']['topLevelComment']['id'],\n",
        "                    'text': top_comment['textDisplay'],\n",
        "                    'author': top_comment['authorDisplayName'],\n",
        "                    'like_count': top_comment.get('likeCount', 0),\n",
        "                    'published_at': top_comment['publishedAt'],\n",
        "                    'reply_count': item['snippet'].get('totalReplyCount', 0),\n",
        "                    'collected_at': datetime.now().isoformat()\n",
        "                }\n",
        "                video_comments.append(comment_data)\n",
        "\n",
        "            all_comments.extend(video_comments)\n",
        "            print(f\"  ✅ {len(video_comments)}개 댓글 수집 완료\")\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        except HttpError as e:\n",
        "            error_message = str(e)\n",
        "            if 'commentsDisabled' in error_message:\n",
        "                print(\"  ⚠️ 댓글이 비활성화된 영상입니다.\")\n",
        "            elif 'quotaExceeded' in error_message:\n",
        "                print(\"  🛑 API 사용량 초과! 댓글 수집을 중단합니다.\")\n",
        "                break\n",
        "            else:\n",
        "                print(f\"  ❌ API 오류: {error_message}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ 댓글 수집 중 알 수 없는 오류: {e}\")\n",
        "\n",
        "    print(f\"\\n댓글 수집 완료! 총 {len(all_comments)}개 댓글 수집\")\n",
        "    return pd.DataFrame(all_comments)\n",
        "\n",
        "def preprocess_comments(comments_df):\n",
        "    \"\"\"\n",
        "    댓글 데이터프레임을 받아 기본적인 전처리를 수행합니다.\n",
        "\n",
        "    Args:\n",
        "        comments_df (pd.DataFrame): 원본 댓글 데이터프레임\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: 필요한 컬럼만 선택하고 텍스트의 양끝 공백을 제거한 데이터프레임\n",
        "    \"\"\"\n",
        "    if comments_df.empty:\n",
        "        print(\"댓글 데이터가 없어 데이터 정리를 건너뜁니다.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # 필요한 컬럼만 선택하여 새로운 데이터프레임 생성\n",
        "    processed_df = comments_df[[\n",
        "        'text', 'author', 'like_count', 'video_title'\n",
        "    ]].copy()\n",
        "\n",
        "    # 'text' 컬럼의 양쪽 끝 공백 제거 (효율적인 pandas 방식)\n",
        "    processed_df['text'] = processed_df['text'].str.strip()\n",
        "\n",
        "    print(f\"총 {len(processed_df)}개의 댓글 처리 완료!\")\n",
        "    return processed_df\n",
        "\n",
        "def generate_and_save_ai_report(\n",
        "    genai_client,\n",
        "    videos_df,\n",
        "    comments_df,\n",
        "    meaningful_comments,\n",
        "    channel_name,\n",
        "    prompt,\n",
        "    save_path=SAVE_PATH,\n",
        "    model_name=\"gemini-2.0-flash\"\n",
        "):\n",
        "    \"\"\"\n",
        "    영상과 댓글 데이터를 기반으로 AI 분석 리포트를 생성하고 파일로 저장합니다.\n",
        "\n",
        "    Args:\n",
        "        genai_client: Gemini AI 클라이언트 객체\n",
        "        model_name (str): 사용할 Gemini 모델 이름 (예: 'gemini-1.5-flash')\n",
        "        videos_df (pd.DataFrame): 분석할 영상 정보 데이터프레임\n",
        "        comments_df (pd.DataFrame): 전체 댓글 데이터프레임 (통계용)\n",
        "        meaningful_comments (list): AI에게 전달할 주요 댓글 리스트\n",
        "        channel_name (str): 분석할 채널의 이름\n",
        "        view_threshold (int): 분석 조건에 명시할 조회수 기준\n",
        "        save_path (str): 분석 리포트 파일을 저장할 경로\n",
        "\n",
        "    Returns:\n",
        "        str: 저장된 파일의 경로. 실패 시 None 반환.\n",
        "    \"\"\"\n",
        "    if videos_df.empty or comments_df.empty:\n",
        "        print(\"영상 또는 댓글 데이터가 부족하여 AI 분석을 건너뜁니다.\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\n=================================================================\")\n",
        "    print(\" AI 분석 리포트 생성 시작\")\n",
        "    print(\"=================================================================\")\n",
        "    print(\"AI 분석용 데이터 준비 중...\")\n",
        "\n",
        "    # 영상 정보를 텍스트로 정리\n",
        "    video_summary = []\n",
        "    for _, video in videos_df.iterrows():\n",
        "        summary = f\"\"\"제목: {video['title']}\n",
        "조회수: {video['view_count']:}회\n",
        "좋아요: {video['like_count']:}개\n",
        "댓글: {video['comment_count']:}개\n",
        "업로드: {video['upload_date']}\n",
        "재생시간: {video['duration']}\"\"\"\n",
        "        video_summary.append(summary)\n",
        "    videos_text = '\\n\\n'.join(video_summary)\n",
        "\n",
        "    # 댓글 정보를 텍스트로 정리\n",
        "    comments_for_ai = []\n",
        "\n",
        "    for _, comment in meaningful_comments.iterrows():\n",
        "      # 댓글을 읽기 쉽게 정리\n",
        "      comment_line = f\"[{comment['like_count']} LIKES ] {comment['author']}: {comment['text']}\"\n",
        "      comments_for_ai.append(comment_line)\n",
        "    # AI에게 분석을 요청할 프롬프트 작성\n",
        "    analysis_prompt = f\"\"\"YouTube 채널 '{channel_name}' 분석 데이터입니다.\n",
        "방송 PD와 콘텐츠 기획자를 위한 실용적인 분석을 해주세요.\n",
        "\n",
        "=== 분석 대상 ===\n",
        "채널: {channel_name}\n",
        "\n",
        "=== 인기 영상 성과 ===\n",
        "{videos_text}\n",
        "\n",
        "=== 시청자 댓글 반응 (댓글 좋아요 순) ===\n",
        "{comments_for_ai}\n",
        "\n",
        "{prompt}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        print(f\"AI 분석 요청 중... (모델: {model_name})\")\n",
        "        response = genai_client.models.generate_content(\n",
        "            model=model_name, # 모델 이름 형식에 맞게 수정\n",
        "            contents=analysis_prompt\n",
        "        )\n",
        "        ai_analysis = response.text\n",
        "\n",
        "        print(\"✅ AI 분석 완료!\")\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"AI 분석 결과\")\n",
        "        print(\"=\"*60)\n",
        "        print(ai_analysis)\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # AI 분석 결과를 파일로 저장\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        analysis_filename = f\"ai_analysis_{channel_name}_{timestamp}.txt\"\n",
        "        analysis_path = os.path.join(save_path, analysis_filename)\n",
        "\n",
        "        with open(analysis_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(f\"=== {channel_name} AI 분석 리포트 ===\\n\")\n",
        "            f.write(f\"분석 일시: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "            f.write(f\"분석 영상: {len(videos_df)}개\\n\")\n",
        "            f.write(f\"분석 댓글: {len(comments_df)}개\\n\")\n",
        "            f.write(f\"AI 전달 댓글: {len(meaningful_comments)}개\\n\\n\")\n",
        "            f.write(\"=\"*60 + \"\\n\")\n",
        "            f.write(ai_analysis)\n",
        "\n",
        "        print(f\"\\n✅ AI 분석 결과 저장 완료: {analysis_path}\")\n",
        "        return analysis_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ AI 분석 중 오류 발생: {e}\")\n",
        "        print(\"API 키를 확인하거나 나중에 다시 시도해주세요.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def retrieve_channel_videos(channel_name,  sort_by='popular', start_date=None, end_date=None, limit=MAX_VIDEOS):\n",
        "  channel_id = get_channel_id_by_handle(channel_name)\n",
        "  videos = get_all_channel_videos(youtube, channel_id)\n",
        "  videos = analyze_channel_videos(videos, sort_by, start_date, end_date, limit)\n",
        "  videos_df = convert_dataframe_from_video_data(videos)\n",
        "  return videos_df\n",
        "\n",
        "def save_raw_data(videos_df, comments_df):\n",
        "  timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "  if not videos_df.empty:\n",
        "    video_filename = f\"videos_{timestamp}.csv\"\n",
        "    video_path = os.path.join(SAVE_PATH, video_filename)\n",
        "    # 한글이 깨지지 않도록 utf-8-sig 인코딩 사용\n",
        "    videos_df.to_csv(video_path, index=False, encoding='utf-8-sig')\n",
        "    print(f\"영상 데이터 저장: {video_filename}\")\n",
        "\n",
        "  # 댓글 정보 저장 (CSV 파일로)\n",
        "  if not comments_df.empty:\n",
        "      comments_filename = f\"comments_{timestamp}.csv\"\n",
        "      comments_path = os.path.join(SAVE_PATH, comments_filename)\n",
        "      comments_df.to_csv(comments_path, index=False, encoding='utf-8-sig')\n",
        "      print(f\"댓글 데이터 저장: {comments_filename}\")\n",
        "\n",
        "  print(f\"저장 완료! 파일 위치: {SAVE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-W-OuLcuGAr",
        "outputId": "71216562-f8eb-465f-c0f3-8002659f48fe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "설정 완료!\n",
            "저장 위치: /content/drive/MyDrive/youtube_analysis/\n",
            "YouTube API 연결 성공!\n",
            "Gemini AI 연결 성공!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"\"\"\n",
        "=== 분석 요청사항 ===\n",
        "다음 관점에서 분석해주세요:\n",
        "1. 현재 인기 영상의 성공 요인 (제목 패턴, 영상 길이, 콘텐츠 스타일)\n",
        "2. 댓글을 통한 시청자 반응 분석 (만족도, 주요 관심사, 긍정/부정 의견)\n",
        "3. 실행 가능한 콘텐츠 전략 제안 (기획 아이디어 3가지, 참여도 증진 방안)\n",
        "4. 주의사항 및 개선점\n",
        "\n",
        "방송 현업에서 바로 쓸 수 있는 구체적이고 실용적인 조언을 한국어로 해주세요.\n",
        "댓글 내용을 근거로 구체적인 예시를 들어 설명해주세요.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7oVhYND041LK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "channel_name = \"@syukaworld\" #분석하고자 하는 채널 이름\n",
        "sort_by = \"latest\" # 비디오 정렬 방법 latest, popular\n",
        "start_date = None # 필터 시작 날짜, 예: 2025-09-01\n",
        "end_date = None # 필터 끝 날짜, 예: 2025-09-18\n",
        "num_videos_to_analyze = 10 #분석하고자 하는 비디오 개수\n",
        "max_comments_per_video = 20 # 수집할 비디오 당 댓글 개수\n",
        "model_name = \"gemini-2.5-flash\" #제미나이 모델\n",
        "\n",
        "videos_df = retrieve_channel_videos(channel_name,sort_by=sort_by, start_date=start_date, end_date=end_date, limit=num_videos_to_analyze)\n",
        "comments_df = collect_video_comments(videos_df, max_comments_per_video=max_comments_per_video)\n",
        "save_raw_data(videos_df, comments_df)\n",
        "meaningful_comments = preprocess_comments(comments_df)\n",
        "generate_and_save_ai_report(genai_client, videos_df, comments_df, meaningful_comments, channel_name, prompt, model_name=model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jSQZ8PVyyD8H",
        "outputId": "01dc52b6-9fab-4de0-ad8a-a7ed305234df"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=================================================================\n",
            "영상 댓글 수집 시작\n",
            "=================================================================\n",
            "\n",
            "[1/10] '미국을 반으로 갈라버린, 인플루언서 암살 사건...' 댓글 수집 중...\n",
            "  ✅ 20개 댓글 수집 완료\n",
            "\n",
            "[2/10] '초유의 한국인 근로자 추방사태...' 댓글 수집 중...\n",
            "  ✅ 20개 댓글 수집 완료\n",
            "\n",
            "[3/10] '코스피 역사상 최고점, 황스피의 시대가 왔다....' 댓글 수집 중...\n",
            "  ✅ 20개 댓글 수집 완료\n",
            "\n",
            "[4/10] '위기를 맞은 '포토샵 기업', 어도비...' 댓글 수집 중...\n",
            "  ✅ 20개 댓글 수집 완료\n",
            "\n",
            "[5/10] '한국은행 파격제안, '규제 풀고 택시 면허 사서 없애라'...' 댓글 수집 중...\n",
            "  ✅ 20개 댓글 수집 완료\n",
            "\n",
            "[6/10] '1%가 75%의 갈등을 생산하는 시대...' 댓글 수집 중...\n",
            "  ✅ 20개 댓글 수집 완료\n",
            "\n",
            "[7/10] ''노란 봉투법' 시행 확정, 무슨 법인가...' 댓글 수집 중...\n",
            "  ✅ 20개 댓글 수집 완료\n",
            "\n",
            "[8/10] '반미 연합 3대장, 전면에 나선 김정은...' 댓글 수집 중...\n",
            "  ✅ 20개 댓글 수집 완료\n",
            "\n",
            "[9/10] '물 위를 날아다니는 소금쟁이의 비밀...' 댓글 수집 중...\n",
            "  ✅ 20개 댓글 수집 완료\n",
            "\n",
            "[10/10] '세상에 공짜 점심은 없다....' 댓글 수집 중...\n",
            "  ✅ 20개 댓글 수집 완료\n",
            "\n",
            "댓글 수집 완료! 총 200개 댓글 수집\n",
            "영상 데이터 저장: videos_20250918_012150.csv\n",
            "댓글 데이터 저장: comments_20250918_012150.csv\n",
            "저장 완료! 파일 위치: /content/drive/MyDrive/youtube_analysis/\n",
            "총 200개의 댓글 처리 완료!\n",
            "\n",
            "=================================================================\n",
            " AI 분석 리포트 생성 시작\n",
            "=================================================================\n",
            "AI 분석용 데이터 준비 중...\n",
            "AI 분석 요청 중... (모델: gemini-2.5-flash)\n",
            "✅ AI 분석 완료!\n",
            "\n",
            "============================================================\n",
            "AI 분석 결과\n",
            "============================================================\n",
            "'@syukaworld' 채널 분석 데이터를 바탕으로 방송 PD와 콘텐츠 기획자를 위한 실용적인 분석 및 제안을 드립니다.\n",
            "\n",
            "---\n",
            "\n",
            "### **1. 현재 인기 영상의 성공 요인 분석**\n",
            "\n",
            "**가. 제목 패턴:**\n",
            "'@syukaworld' 채널의 인기 영상 제목들은 시청자들의 이목을 즉각적으로 사로잡는 데 매우 효과적입니다.\n",
            "\n",
            "*   **극적이고 논쟁적인 키워드 활용:** \"미국을 반으로 갈라버린, 인플루언서 암살 사건\", \"초유의 한국인 근로자 추방사태\", \"코스피 역사상 최고점\", \"위기를 맞은 '포토샵 기업', 어도비\", \"'노란 봉투법' 시행 확정\" 등은 사건의 심각성, 파격성, 최신성을 강조하여 클릭을 유도합니다. 특히 \"미국을 반으로 갈라버린\" 같은 수식어는 시청자의 궁금증을 극대화합니다.\n",
            "*   **사회/경제적 이슈의 직관적 제시:** 제목만으로도 어떤 사회적, 경제적 문제를 다루는지 명확히 알 수 있게 합니다. 이는 시청자가 자신과 관련된 문제라고 인식하게 하여 몰입도를 높입니다.\n",
            "*   **호기심 자극 및 의미 부여:** \"1%가 75%의 갈등을 생산하는 시대\", \"세상에 공짜 점심은 없다.\"와 같이 추상적이면서도 시사하는 바가 큰 문구는 시청자로 하여금 영상이 제시할 통찰력에 대한 기대를 갖게 합니다.\n",
            "*   **예외적 성공 (\"소금쟁이\"):** \"물 위를 날아다니는 소금쟁이의 비밀\"처럼 채널의 주력 콘텐츠와 다른 과학/교양 주제 영상도 높은 조회수를 기록했습니다. 이는 진행자 '슈카'에 대한 팬덤이 두터움을 보여주며, 채널이 '슈카'라는 브랜드 자체의 힘으로 다양한 콘텐츠를 소화할 수 있음을 시사합니다.\n",
            "\n",
            "**나. 영상 길이:**\n",
            "대부분의 인기 영상은 **17분에서 38분 사이**의 비교적 긴 재생시간을 가지고 있습니다.\n",
            "\n",
            "*   **심층 분석 및 충분한 정보 제공:** 이러한 길이는 단순 뉴스 전달을 넘어, 복잡한 사회/경제 이슈에 대한 배경 설명, 다양한 관점 분석, 미래 예측 등을 심층적으로 다루는 데 적합합니다. 시청자들이 겉핥기식 정보가 아닌, 깊이 있는 지식을 얻고자 하는 니즈를 충족시켜줍니다.\n",
            "*   **유튜브 알고리즘에 긍정적:** 긴 재생시간에도 불구하고 높은 조회수를 유지한다는 것은 시청 지속 시간이 높다는 의미로, 유튜브 알고리즘에 긍정적인 신호로 작용하여 더 많은 노출을 가져올 수 있습니다. 이는 채널의 충성도 높은 시청자층이 긴 호흡의 콘텐츠를 충분히 소화할 역량을 갖추고 있음을 보여줍니다.\n",
            "\n",
            "**다. 콘텐츠 스타일:**\n",
            "제공된 정보와 댓글을 종합해 볼 때, 다음과 같은 스타일이 성공 요인으로 분석됩니다.\n",
            "\n",
            "*   **이슈 중심의 시의성:** 최근 발생했거나 사회적으로 뜨거운 논쟁이 되는 이슈들을 빠르게 다룹니다. (예: '노란 봉투법', '한국인 근로자 추방사태', '미국 인플루언서 암살 사건')\n",
            "*   **진행자의 전문성 및 유머 감각:** 댓글에서 '슈카형', '슈카'에 대한 직접적인 언급이 많으며, '생물 유튜버 복귀', '소금빵의 소심한 반격' 등 유머러스한 반응이 많아 진행자의 전문성과 함께 특유의 유머나 입담이 콘텐츠의 매력으로 작용하고 있음을 알 수 있습니다.\n",
            "*   **논쟁적 주제 불사:** 사회적 논쟁의 중심에 있는 주제들을 과감하게 다루며 시청자들의 적극적인 참여와 토론을 유도합니다. 이는 댓글 수 폭발로 이어집니다.\n",
            "*   **시청자 공감대 형성:** '어도비' 영상처럼 일반인들이 체감하는 불만이나 '한국은행 택시 면허' 영상처럼 시대 변화에 대한 갈증을 건드리는 콘텐츠는 높은 공감대와 압도적인 긍정/부정 의견 일치로 이어집니다. (예: \"@수정김-t8p: 디자이너 입장에서 어도비 사용자들 상대로 너무 악질임 대체재가 없다는 이유로 믿도끝도 없이 갑질하고 가격 미친듯 올리고\")\n",
            "\n",
            "---\n",
            "\n",
            "### **2. 댓글을 통한 시청자 반응 분석**\n",
            "\n",
            "**가. 만족도:**\n",
            "전반적으로 시청자들은 채널의 콘텐츠에 대해 높은 관심과 참여 의지를 보이며, 이는 높은 조회수와 댓글 수로 이어집니다. 특히 사회/경제적 이슈에 대한 심도 있는 분석에 대해 만족도가 높습니다.\n",
            "\n",
            "**나. 주요 관심사:**\n",
            "\n",
            "*   **사회/경제/정치적 현안:** '노란 봉투법', '한국은행 택시 면허', '복지 지출 문제', '미국 인플루언서 암살 사건', '반미 연합' 등 현재 한국 및 국제 사회의 주요 이슈에 대한 관심이 압도적입니다. 이는 시청자들이 현실과 밀접한 관련이 있는 정보를 얻고자 함을 보여줍니다.\n",
            "*   **정보의 신뢰성 및 투명성:** '찰리 커크' 관련 논란 영상에서 \"외신 보도 어디를 바탕으로 했는지도 알려주셔야죠\", \"출처도 같이 쓰세요\"와 같은 댓글이 다수 등장했습니다. 이는 시청자들이 단순히 정보를 받아들이는 것을 넘어, 정보의 근거와 진행자의 중립성에 대한 높은 기대를 가지고 있음을 나타냅니다. 특히 논쟁적인 주제일수록 투명한 정보 공개에 대한 요구가 강합니다.\n",
            "*   **진행자의 스탠스 및 역할:** 진행자인 슈카의 개인적인 견해나 방송 태도에 대한 관심이 높습니다. \"이제 중도인 척도 안하나봐요?\", \"심하게 편향적인 영포티 ㅋㅋㅋㅋㅋ\"와 같은 비판과 함께, \"소신 발언하기 쉽지 않은 위치인데도 이런 목소리를 내주는 게 고맙네\"와 같은 옹호 의견이 공존합니다. 이는 진행자가 콘텐츠의 핵심이자 채널의 브랜딩 그 자체임을 보여줍니다.\n",
            "*   **댓글 문화 및 온라인 여론의 병폐:** \"1%가 75%의 갈등을 생산하는 시대\" 영상 댓글에서 \"정상인들은 댓글보고 반박할려고 2줄 쓰면 내가 이 당연할걸 설명하고 있는 모습에서 현타와서 2줄 써져있는 상태에서 창을 닫거나 다른 영상 클릭해버림\", \"커뮤 여론 조작은 정말 쉬움\" 등 과열된 온라인 여론과 댓글 문화에 대한 비판적 인식이 시청자들 사이에서 강하게 나타나고 있습니다.\n",
            "*   **개인의 불만/공감대:** '어도비' 영상에서는 불합리한 구독 정책에 대한 디자이너들의 공통된 불만이 폭발적으로 터져 나왔습니다. (예: \"@errorwindows-1: 포토샵 이용료 너무 터무니없이 비싸고 프로그램도 느려터졌고 대체할만한 프로그램이 나왔으면 좋겠다.\")\n",
            "\n",
            "**다. 긍정/부정 의견:**\n",
            "\n",
            "*   **긍정적 의견:**\n",
            "    *   **용기와 소신에 대한 지지:** \"소신 발언하기 쉽지 않은 위치인데도 이런 목소리를 내주는 게 고맙네\" (@IlIIlllIIIIllIlI)와 같이 사회적 논쟁을 다루는 진행자의 용기에 대한 지지가 높습니다.\n",
            "    *   **깊이 있는 분석에 대한 만족:** \"이 형이 힙합인 거 같다\" (@chrisyou740), \"진짜 뜨거운 거 들고오셨네 ㅎㅅㅎ\" (@brickism_) 등 날카로운 분석과 시의적절한 주제 선정에 대한 긍정적 평가가 많습니다.\n",
            "    *   **진행자의 매력:** '소금쟁이' 영상에서 \"돌아온 생물 유튜버 🎉🎉\" (@또이또이-m9p) 와 같은 반응은 슈카의 개인적 매력과 과거 콘텐츠에 대한 애정을 보여줍니다.\n",
            "*   **부정적 의견:**\n",
            "    *   **편향성/중립성 논란:** '찰리 커크' 영상 관련하여 \"이제 중도인 척도 안하나봐요?\", \"심하게 편향적인 영포티 ㅋㅋㅋㅋㅋ\" (@유승준-c1p) 등 진행자의 특정 정치적/사회적 스탠스에 대한 비판이 강하게 제기되었습니다.\n",
            "    *   **정보 왜곡/조롱 논란:** \"라이브에서 조롱 신나게 해놓고 희화화하려는 의도 없음 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ\", \"생방때 얘기 그대로 쳐 올리든가 ㅋㅋ 존나 졸렬하다 진짜 ㅋㅋㅋ\" (@u1u4801), \"찰리커크 고인모독영상 잘봤습니다\" (@항상건강한남자) 등 라이브 발언 및 편집에 대한 불만, 특정 인물에 대한 조롱성 언급에 대한 비난이 채널의 신뢰도에 부정적인 영향을 미칠 수 있음을 시사합니다.\n",
            "    *   **불투명한 대응 비판:** 논란이 된 영상의 삭제/비공개 처리(\"조롱한거 SNS 보고 왔는데 삭제 했네...\", \"유튜브 정지는 무섭나보네.. ㅋㅋ\")에 대해서도 시청자들은 비판적인 시각을 보였습니다.\n",
            "\n",
            "---\n",
            "\n",
            "### **3. 실행 가능한 콘텐츠 전략 제안**\n",
            "\n",
            "**가. 기획 아이디어 3가지:**\n",
            "\n",
            "1.  **[Hot Issue] '갈등의 민낯' 심층 해부 시리즈:**\n",
            "    *   **아이디어:** 현재 시청자 댓글에서 가장 첨예하게 대립하고 있는 사회/경제적 이슈(예: '노란 봉투법', '한국인 근로자 추방사태' 관련 노동 정책, 복지 문제 등)를 선정하여, 단순히 사건을 전달하는 것을 넘어 **왜 이러한 갈등이 발생하는지, 각 주장의 배경과 논리, 그리고 실제 해외 사례는 어떠한지** 등을 객관적인 데이터와 전문가 인터뷰(가상)를 통해 다각도로 분석하는 심층 시리즈를 기획합니다.\n",
            "    *   **목표:** 시청자들이 '좌우 타령하는 애들이 나라 망쳐먹는 중' (@djsneixnf)과 같은 피로감을 느끼는 현 상황에서, 복잡한 갈등의 본질을 이해하고 스스로 판단할 수 있는 근거를 제공하여 채널의 신뢰도를 높입니다.\n",
            "    *   **예시:** \"노란 봉투법 논쟁, 한국 사회의 구조적 갈등인가 선진국으로 가는 진통인가? - 해외 노조법 사례와 비교 분석\" (댓글: \"@user-nt7uu3fz6t: 택시부터 저렇게 다 회수하고 교통문화 바꿔야함, 어차피 자율주행 뜨면 다 사라질업종임 갈아엎지않으면 아무런 변화가 없음. 옆에 세계 정점찍고 내려온 일본만 가도 교통질서 택시부터 잘지키고 서비스도 좋음\")\n",
            "    *   **차별점:** 기존의 일방적인 정보 전달에서 벗어나, 찬반 양측의 논리를 균형 있게 다루고, 전문가의 심층 의견을 덧붙여 단순한 이슈 팔이가 아닌 '지적 탐구'의 장을 만듭니다.\n",
            "\n",
            "2.  **[Audience Participation] '시청자가 묻고 슈카가 답한다' - 팩트체크/심층 질문 코너:**\n",
            "    *   **아이디어:** 특정 논란(특히 진행자의 스탠스나 정보의 출처에 대한 의문이 제기된 경우, 예: '찰리 커크' 영상 관련)에 대해 시청자들이 직접 질문을 올리고, 슈카가 그 질문에 대해 명확한 **근거(외신 보도 원문, 통계 자료 등)를 제시하며 답변하는 정기 코너**를 신설합니다. 커뮤니티 탭을 통해 질문을 받고, 영상에서 우수 질문을 선정하여 답변하는 형식입니다.\n",
            "    *   **목표:** 정보의 신뢰성에 대한 시청자의 갈증을 해소하고, 채널의 투명성을 높여 논란을 사전에 방지합니다. 또한 시청자 참여를 통해 소통을 강화하고 충성도를 높일 수 있습니다.\n",
            "    *   **예시:** \"이번 '미국 인플루언서 암살 사건' 보도, 시청자 질문에 답합니다 - 찰리 커크 발언 원문과 맥락 분석 및 외신 보도 출처 공개\" (댓글: \"@jyk3732: 죄송한데 외신 보도 어디를 바탕으로 했는지도 알려주셔야죠\", \"@oliverjun8061: 찰리커크가 명백한 백인우월주의자라는 말씀은 무슨 근거로 하신건가요?\")\n",
            "    *   **차별점:** 논란 발생 시 비공개 처리 등 수동적 대처가 아닌, 적극적인 소통과 팩트 체크를 통해 채널의 신뢰도를 재정립합니다.\n",
            "\n",
            "3.  **[Relax/Education] '본캐의 귀환' - 세상의 모든 '비밀' 탐구:**\n",
            "    *   **아이디어:** \"물 위를 날아다니는 소금쟁이의 비밀\"처럼 채널의 메인 경제/시사 콘텐츠에서 벗어나, 슈카의 원래 전공이나 개인적 호기심이 발동하는 **과학, 자연, 역사, 문화 등 다양한 분야의 흥미로운 주제**를 다루는 콘텐츠를 기획합니다. 월 1~2회 정도의 빈도로 시청자들에게 신선한 재미와 지적 자극을 제공합니다.\n",
            "    *   **목표:** 시청자들의 \"생물 유튜버 복귀 🎉🎉\" (@또이또이-m9p), \"수상할 정도로 소금에 집착하는 경제유튜버\" (@안드로메다-f3z)와 같은 긍정적 반응에서 알 수 있듯이, 슈카의 개인적 매력과 지적 호기심을 발산하며 채널의 스펙트럼을 확장합니다. 과열된 사회 이슈 영상들 사이에서 템포 조절 역할을 하며 시청자들에게 휴식과 새로운 지적 자극을 제공합니다.\n",
            "    *   **예시:** \"AI가 아직 넘볼 수 없는 인간의 '창의성'은 무엇인가? - 예술과 과학의 경계에서\", \"인류를 바꾼 작은 '씨앗'의 경제사\" (댓글: 어도비 대체재에 대한 관심도 높으므로, 기술 발전 관련 호기심을 자극할 수 있습니다.)\n",
            "    *   **차별점:** 채널의 '재미' 요소를 강화하고, 슈카의 다방면 지식과 재능을 보여주며 채널의 브랜딩을 더욱 풍성하게 만듭니다.\n",
            "\n",
            "**나. 참여도 증진 방안:**\n",
            "\n",
            "1.  **댓글 챌린지 및 질문 공모전 정례화:**\n",
            "    *   영상 말미에 다음 영상 주제와 관련하여 시청자들의 궁금증이나 질문을 댓글로 받겠다고 공지합니다. 우수 질문을 선정하여 다음 영상에서 답변하거나, 질문한 시청자를 영상에 언급하여 소속감과 참여율을 높입니다.\n",
            "2.  **커뮤니티 탭 적극 활용:**\n",
            "    *   영상이 다룬 이슈에 대한 추가 정보, 시청자 설문조사, 팩트체크 자료 등을 커뮤니티 탭에 정기적으로 게시하여 영상 외적으로도 소통 창구를 확장하고 시청자 의견을 수렴합니다. 라이브 방송 전 시청자 질문을 미리 받는 등의 활용도 좋습니다.\n",
            "3.  **라이브 방송 시 '건전한 토론 문화' 조성 노력:**\n",
            "    *   라이브 방송은 즉각적인 소통에 효과적이지만, 댓글 반응에서 보듯이 과도한 '좌우 이념 갈등'이나 '진행자 조롱' 등의 부정적인 반응이 과열될 수 있습니다. 라이브 시 특정 논란에 대한 진행자의 명확한 입장 표명(정확한 정보 기반)과 함께, 과도한 비난이나 편향된 의견은 정중하게 제지하는 등 진행자의 적극적인 '댓글 중재자' 역할이 필요합니다. 이를 통해 건전한 토론 문화를 유도하고 채널의 긍정적인 이미지를 구축합니다.\n",
            "\n",
            "---\n",
            "\n",
            "### **4. 주의사항 및 개선점**\n",
            "\n",
            "1.  **정보의 정확성 및 출처 명시 강화:**\n",
            "    *   '찰리 커크' 관련 논란 영상에서 \"외신 보도 어디를 바탕으로 했는지도 알려주셔야죠\", \"출처도 같이 쓰세요\"와 같이 정보의 신뢰성과 출처 표기에 대한 시청자들의 요구가 매우 높습니다. 특히 정치/사회적 이슈는 사실 관계 확인이 생명입니다.\n",
            "    *   **개선점:** 모든 영상 제작 시 **복수의 신뢰할 수 있는 출처를 교차 확인**하고, 영상 내에 **명확하게 출처를 표기**하는 것을 원칙으로 삼아야 합니다. 논란 발생 시에는 즉각적인 **해명 및 정정 영상 또는 공지**를 통해 투명하게 대응하여 신뢰도를 유지해야 합니다. '삭제/비공개'는 오해를 키울 수 있습니다.\n",
            "\n",
            "2.  **'중립성' 유지에 대한 노력과 오해 해소:**\n",
            "    *   시청자들은 진행자에게 '중립성'을 기대하지만, \"중도인 척도 안하나봐요?\", \"심하게 편향적인 영포티 ㅋㅋㅋㅋㅋ\" 등 특정 스탠스에 대한 비판도 존재합니다. 완벽한 중립은 어렵더라도 '객관적인 사실'을 기반으로 '다양한 관점'을 제시하려는 노력은 필수입니다.\n",
            "    *   **개선점:** 특정 주장을 옹호하는 것처럼 비춰질 수 있는 표현이나 편집은 신중해야 합니다. 자신의 견해를 밝히더라도, 그 근거와 함께 **'하나의 관점'임을 명확히** 하여 시청자 판단에 맡기는 태도를 보여주는 것이 좋습니다.\n",
            "\n",
            "3.  **과도한 논란 조장 자제 및 윤리적 콘텐츠 기획:**\n",
            "    *   논쟁적 주제는 조회수와 참여도를 높이는 효과가 크지만, '찰리 커크' 건처럼 특정 인물을 '조롱'하거나 사실을 왜곡했다는 비난은 채널의 신뢰도에 치명적입니다. \"라이브에서 조롱 신나게 해놓고 희화화하려는 의도 없음\", \"생방때 얘기 그대로 쳐 올리든가 ㅋㅋ 존나 졸렬하다 진짜 ㅋㅋ\" 등의 댓글을 간과해서는 안 됩니다.\n",
            "    *   **개선점:** 조회수만을 위한 **과도한 어그로나 특정 인물/집단에 대한 비하, 조롱은 지양**해야 합니다. 콘텐츠 기획 시 사회적 영향력을 고려한 **높은 윤리적 기준**을 적용하고, 사실에 기반한 비판과 분석에 집중해야 합니다.\n",
            "\n",
            "4.  **시청자 피드백 수용 및 소통 방식 개선:**\n",
            "    *   논란이 된 '찰리 커크' 영상의 '삭제/편집' 논란은 투명성 문제를 야기했습니다. \"유튜브 정지는 무섭나보네.. ㅋㅋ\" 같은 댓글은 시청자들이 채널의 대응을 부정적으로 인식하고 있음을 보여줍니다.\n",
            "    *   **개선점:** 유사한 상황 발생 시에는 \"내용이 잘못되어 수정했습니다\"와 같이 **명확하게 공지하고 설명**하는 것이 시청자들과의 신뢰 관계 구축에 훨씬 중요합니다. 비판적인 의견이라도 진지하게 경청하고, 가능한 범위 내에서 **피드백을 반영하는 모습**을 보여주는 것이 채널의 장기적인 성장 동력이 될 것입니다.\n",
            "\n",
            "5.  **긍정적인 커뮤니티 환경 조성:**\n",
            "    *   댓글창이 과도한 비난과 갈등으로 변질되는 현상에 대한 시청자들의 우려가 큽니다. (\"진짜 댓글들이 속상합니다. 나라가 망하길 바라는가?\") 이는 '1%가 75%의 갈등을 생산하는 시대' 영상의 주제 의식과도 일맥상통합니다.\n",
            "    *   **개선점:** 진행자가 영상이나 라이브에서 **댓글 문화에 대한 개선 의지를 표명**하거나, 모범적인 댓글을 칭찬하는 등의 방식으로 **긍정적인 커뮤니티 분위기를 유도**할 필요가 있습니다. 건강한 토론과 건설적인 비판이 오가는 환경을 만드는 것이 장기적인 채널 활성화에 기여할 것입니다.\n",
            "\n",
            "---\n",
            "\n",
            "'@syukaworld' 채널은 이미 강력한 브랜드 파워와 충성도 높은 시청자층을 확보하고 있습니다. 위에 제시된 분석과 개선점을 바탕으로 콘텐츠의 질과 운영의 투명성을 더욱 높인다면, 단순한 인기 채널을 넘어 사회적으로 긍정적인 영향력을 미치는 미디어로서 한 단계 더 도약할 수 있을 것입니다.\n",
            "============================================================\n",
            "\n",
            "✅ AI 분석 결과 저장 완료: /content/drive/MyDrive/youtube_analysis/ai_analysis_@syukaworld_20250918_012333.txt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/youtube_analysis/ai_analysis_@syukaworld_20250918_012333.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}